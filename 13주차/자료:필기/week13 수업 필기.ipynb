{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNV/dqiBhji8Q8QmquOO1vr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# week13 - Transformer\n","\n","비교적 근래에 나온 모델 - 지금 가장 핫한 모델\n","\n","2018년에 google에서 발표한 논문에서 나옴\n","\n","**굉장히 중요**\n","\n","기계 번역 모델에 사용되는 것이 Transformer\n","\n","bert -> 에서 decoder만 떼서 gpt\n","\n","LSTM 같은 RNN 모델의 문제점\n","- ?\n","- long dependency 해결 불가능 -> 중간에 forget 됐을 때, 먼 뒤에서 필요로 할 때 사용 불가능\n","\n","<br>\n","\n","Transformer - 3개의 attention 존재\n","\n","**계산하는 식 설명해줌 -> 계산할 줄 알아야 함.**"],"metadata":{"id":"_44PQ6_X75if"}}]}
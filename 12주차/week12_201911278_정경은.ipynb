{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNClisSXb3Te1d+aPgf+FWc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# week12 과제\n","\n","다음 두 가지 중에 하나를 선택하세요.\n","- 제공된 소스 코드의 빈칸 채우기(10점)\n","- 제공된 소스 코드를 수정하여 성능을 개선한 모델 만들기 (최대 15점: 10점 + 가산점 5점)\n","    - ipynb 파일에 성능 개선 아이디어를 서술하고, 실행 결과를 제시\n","\n","\n","hidden_size를 150으로 늘리고, layer를 2로 늘림"],"metadata":{"id":"bNFQj-ASDYmp"}},{"cell_type":"markdown","source":["### layer=1 -> layer=2\n","### hidden_size=100 -> hidden_size=150\n","\n","\n","층과 히든 노드 개수를 늘리면 학습을 더욱 정교하게 할 수 있을 것으로 예상하고, layer 수를 2로, hidden_size를 150으로 늘렸습니다.\n","\n","hidden_size를 높였을 때보다 layer 개수를 늘렸을 때 차이가 더 컸으며,\n","hidden_size는 layer가 1, 2 각각의 경우에 대하여 100, 150, 200로 변화시켜 accuracy를 출력한 결과,\n","layer=2, hidden_size=150일 때 가장 정확도가 높은 것을 확인할 수 있었습니다."],"metadata":{"id":"0WKXr1UQrW1D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdYhWveoDi1S","executionInfo":{"status":"ok","timestamp":1668855877509,"user_tz":-540,"elapsed":3131,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}},"outputId":"6b429169-6158-48e0-a46f-cedb68f7b0e5"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import (DataLoader, TensorDataset)\n","from sklearn.metrics import accuracy_score\n","\n","class BeforeSpacingRNN(nn.Module):\n","\n","    def __init__(self, config): \n","        super(BeforeSpacingRNN, self).__init__()\n","\n","        self.eumjeol_vocab_size = config[\"eumjeol_vocab_size\"]\n","        self.embedding_size = config[\"embedding_size\"]\n","        self.hidden_size = config[\"hidden_size\"]\n","\n","        self.number_of_labels = config[\"number_of_labels\"] \n","        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size, embedding_dim=self.embedding_size, padding_idx=0)\n","\n","        self.dropout = nn.Dropout(config[\"dropout\"]) \n","        \n","        # RNN layer\n","        self.bi_lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n","        self.linear = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_labels)\n","    \n","    def forward(self, inputs):\n","        eumjeol_inputs = self.embedding(inputs)\n","        hidden_outputs, hidden_states = self.bi_lstm(eumjeol_inputs)\n","\n","        hidden_outputs = self.dropout(hidden_outputs)\n","\n","        hypothesis = self.linear(hidden_outputs)\n","\n","        return hypothesis\n","\n","\n","class SpacingRNN(nn.Module):\n","\n","    def __init__(self, config): \n","        super(SpacingRNN, self).__init__()\n","\n","        self.eumjeol_vocab_size = config[\"eumjeol_vocab_size\"]\n","        self.embedding_size = config[\"embedding_size\"]\n","        self.hidden_size = config[\"hidden_size\"]\n","        self.number_of_labels = config[\"number_of_labels\"]\n","        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size, embedding_dim=self.embedding_size, padding_idx=0)\n","        self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","        # RNN layer\n","        self.bi_lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=2, batch_first=True, bidirectional=True) # layer를 하나 늘림 -> num_layer=2\n","        self.linear = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_labels)\n","\n","\n","    def forward(self, inputs):\n","        eumjeol_inputs = self.embedding(inputs)\n","        hidden_outputs, hidden_states = self.bi_lstm(eumjeol_inputs)\n","        hidden_outputs = self.dropout(hidden_outputs)\n","        hypothesis = self.linear(hidden_outputs)\n","\n","        return hypothesis"],"metadata":{"id":"U_r664BiGPrR","executionInfo":{"status":"ok","timestamp":1668856263738,"user_tz":-540,"elapsed":3,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["def read_datas(file_path):\n","    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n","        lines = inFile.readlines()\n","\n","    datas = []\n","\n","    for line in lines:\n","        pieces = line.strip().split(\"\\t\")\n","        eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n","        datas.append((eumjeol_sequence, label_sequence))\n","\n","    return datas\n","\n","def read_vocab_data(eumjeol_vocab_data_path):\n","    label2idx, idx2label = {\"<PAD>\":0, \"B\":1, \"I\":2}, {0:\"<PAD>\", 1:\"B\", 2:\"I\"}\n","    eumjeol2idx, idx2eumjeol = {}, {}\n","\n","    with open(eumjeol_vocab_data_path, \"r\", encoding=\"utf8\") as inFile:\n","        lines = inFile.readlines()\n","\n","    for line in lines:\n","        eumjeol = line.strip()\n","        eumjeol2idx[eumjeol] = len(eumjeol2idx)\n","        idx2eumjeol[eumjeol2idx[eumjeol]] = eumjeol\n","\n","    return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n","\n","def load_dataset(config):\n","    datas = read_datas(config[\"input_data\"])\n","    eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_data(config[\"eumjeol_vocab\"])\n","\n","    eumjeol_features, eumjeol_feature_lengths, label_features = [], [], []\n","\n","    for eumjeol_sequence, label_sequence in datas:\n","        eumjeol_feature = [eumjeol2idx[eumjeol] for eumjeol in eumjeol_sequence]\n","        label_feature = [label2idx[label] for label in label_sequence]\n","\n","        eumjeol_feature_length = len(eumjeol_feature)\n","\n","        eumjeol_feature += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n","        label_feature  += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n","\n","        eumjeol_features.append(eumjeol_feature)\n","        eumjeol_feature_lengths.append(eumjeol_feature_length)\n","        label_features.append(label_feature)\n","\n","    eumjeol_features = torch.tensor(eumjeol_features, dtype=torch.long)\n","    eumjeol_feature_lengths = torch.tensor(eumjeol_feature_lengths, dtype=torch.long)\n","    label_features = torch.tensor(label_features, dtype=torch.long)\n","\n","    return eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label"],"metadata":{"id":"chaSL1htGf9r","executionInfo":{"status":"ok","timestamp":1668856265674,"user_tz":-540,"elapsed":273,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["def train(config):\n","    model = SpacingRNN(config).cuda()\n","\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    train_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(config[\"epoch\"]):\n","\n","        model.train()\n","        costs = []\n","\n","        for step, batch in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","            hypothesis = model(inputs)\n","\n","            cost = loss_func(hypothesis.reshape(-1, len(label2idx)), labels.flatten())\n","\n","            cost.backward()\n","            optimizer.step()\n","\n","            costs.append(cost.data.item())\n","\n","        torch.save(model.state_dict(), os.path.join(output_dir, \"epoch_{0:d}.pt\".format(epoch + 1)))\n","\n","        print(\"Improved Average cost : {}\".format(np.mean(costs)))"],"metadata":{"id":"mS7YGv_pGlrz","executionInfo":{"status":"ok","timestamp":1668856267237,"user_tz":-540,"elapsed":278,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["def make_sentence(inputs, predicts, labels, idx2eumjeol, idx2label):\n","\n","    predict_sentence, correct_sentence = \"\", \"\"\n","\n","    for index in range(len(inputs)):\n","        eumjeol = idx2eumjeol[inputs[index]]\n","        correct_label = idx2label[labels[index]]\n","        predict_label = idx2label[predicts[index]]\n","\n","        if (index == 0):\n","            predict_sentence += eumjeol\n","            correct_sentence += eumjeol\n","            continue\n","\n","        if (predict_label == \"B\"):\n","            predict_sentence += \" \"\n","        predict_sentence += eumjeol\n","\n","        if (correct_label == \"B\"):\n","            correct_sentence += \" \"\n","        correct_sentence += eumjeol\n","\n","    return predict_sentence, correct_sentence\n","\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","def test(config):\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    test_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=1)\n","\n","    model = SpacingRNN(config).cuda()\n","    \n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"model_name\"])))\n","\n","    total_hypothesis, total_labels = [], []\n","\n","    for step, batch in enumerate(test_dataloader):\n","\n","        model.eval()\n","        batch = tuple(t.cuda() for t in batch)\n","\n","        inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","        hypothesis = model(inputs)\n","\n","        hypothesis = torch.argmax(hypothesis, dim=-1)\n","        \n","        input_length = tensor2list(input_lengths[0])\n","        input = tensor2list(inputs[0])[:input_length]\n","        label = tensor2list(labels[0])[:input_length]\n","        hypothesis = tensor2list(hypothesis[0])[:input_length]\n","\n","        total_hypothesis += hypothesis\n","        total_labels += label\n","\n","        if (step < 10):\n","            predict_sentence, correct_sentence = make_sentence(input, hypothesis, label, idx2eumjeol, idx2label)\n","            print(\"정답 : \" + correct_sentence)\n","            print(\"출력 : \" + predict_sentence)\n","            print()\n","\n","    print(\"Improved Accuracy : {}\".format(accuracy_score(total_labels, total_hypothesis)))"],"metadata":{"id":"XlvHtTAtGuY0","executionInfo":{"status":"ok","timestamp":1668856268553,"user_tz":-540,"elapsed":2,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["def trainBefore(config):\n","    model = BeforeSpacingRNN(config).cuda()\n","\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    train_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(config[\"epoch\"]):\n","\n","        model.train()\n","        costs = []\n","\n","        for step, batch in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","            hypothesis = model(inputs)\n","\n","            cost = loss_func(hypothesis.reshape(-1, len(label2idx)), labels.flatten())\n","\n","            cost.backward()\n","            optimizer.step()\n","\n","            costs.append(cost.data.item())\n","\n","        torch.save(model.state_dict(), os.path.join(output_dir_before, \"epoch_{0:d}.pt\".format(epoch + 1)))\n","\n","        print(\"Before Average cost : {}\".format(np.mean(costs)))\n","\n","def testBefore(config):\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    test_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=1)\n","\n","    model = BeforeSpacingRNN(config).cuda()\n","    \n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"model_name\"])))\n","\n","    total_hypothesis, total_labels = [], []\n","\n","    for step, batch in enumerate(test_dataloader):\n","\n","        model.eval()\n","        batch = tuple(t.cuda() for t in batch)\n","\n","        inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","        hypothesis = model(inputs)\n","\n","        hypothesis = torch.argmax(hypothesis, dim=-1)\n","        input_length = tensor2list(input_lengths[0])\n","        input = tensor2list(inputs[0])[:input_length]\n","        label = tensor2list(labels[0])[:input_length]\n","        hypothesis = tensor2list(hypothesis[0])[:input_length]\n","\n","        total_hypothesis += hypothesis\n","        total_labels += label\n","\n","        if (step < 10):\n","            predict_sentence, correct_sentence = make_sentence(input, hypothesis, label, idx2eumjeol, idx2label)\n","            print(\"정답 : \" + correct_sentence)\n","            print(\"출력 : \" + predict_sentence)\n","            print()\n","\n","    print(\"Before Accuracy : {}\".format(accuracy_score(total_labels, total_hypothesis)))"],"metadata":{"id":"EB9E1YqTheZX","executionInfo":{"status":"ok","timestamp":1668857120392,"user_tz":-540,"elapsed":265,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["if(__name__==\"__main__\"):\n","    root_dir = \"/gdrive/My Drive/ml_colab/week12\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    output_dir_before = os.path.join(root_dir, \"outputbefore\")\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    if not os.path.exists(output_dir_before):\n","        os.makedirs(output_dir_before)\n","        \n","\n","\n","    # 개선 전 모델에 대한 config\n","    configBefore = {\"mode\": \"test\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(5),\n","              \"input_data\":os.path.join(root_dir, \"test.txt\"),\n","              \"output_dir_path\":output_dir_before,\n","              \"eumjeol_vocab\": os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n","              \"label_vocab\": os.path.join(root_dir, \"label_vocab.txt\"),\n","              \"eumjeol_vocab_size\": 2458,\n","              \"embedding_size\": 100,\n","              \"hidden_size\": 100,\n","              \"max_length\": 920,\n","              \"number_of_labels\": 3,\n","              \"epoch\":5,\n","              \"batch_size\":64,\n","              \"dropout\":0.3\n","              }\n","\n","    # 개선 후 모델에 대한 config\n","    config = {\"mode\": \"test\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(5),\n","              \"input_data\":os.path.join(root_dir, \"test.txt\"),\n","              \"output_dir_path\":output_dir,\n","              \"eumjeol_vocab\": os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n","              \"label_vocab\": os.path.join(root_dir, \"label_vocab.txt\"),\n","              \"eumjeol_vocab_size\": 2458,\n","              \"embedding_size\": 100,\n","              \"hidden_size\": 150, # 원래: 100 -> 150\n","              # 100, 150, 200 으로 수정하며 결과 확인 결과, 150일 때 accuracy가 가장 높은 수치를 보임을 확인했음.\n","              \"max_length\": 920,\n","              \"number_of_labels\": 3,\n","              \"epoch\":5,\n","              \"batch_size\":64,\n","              \"dropout\":0.3\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        print(\"[개선 전]\")\n","        print()\n","        trainBefore(configBefore)\n","        print()\n","        print()\n","        print(\"[개선 후]\")\n","        print()\n","        train(config)\n","    else:\n","        print(\"[개선 전]\")\n","        print()\n","        testBefore(configBefore)\n","        print()\n","        print()\n","        print(\"[개선 후]\")\n","        print()\n","        test(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53FDqpiRG9BA","executionInfo":{"status":"ok","timestamp":1668857793869,"user_tz":-540,"elapsed":3469,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}},"outputId":"7e6d2db0-4dbf-447c-87a5-48bbc4e88ef0"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["[개선 전]\n","\n","정답 : 그러고 보니 경리는 윤보혜의 근황에 대해 아는 것이 없어보였다.\n","출력 : 그러고 보니 경리는 윤보혜의 근황에 대해 아는 것이 없어보였다.\n","\n","정답 : 이제 7년 대환란이 눈앞에 닥쳐왔습니다.\n","출력 : 이 제7년 대환란 이 눈앞에 닥쳐왔습니다.\n","\n","정답 : 이 관찰자에게 장치(예를 들면 90의 경사각을 가진 두 거울)를 제공하여, 같은 시각에 A와 B 두 곳을 한꺼번에 관찰할 수 있게 한다.\n","출력 : 이 관찰자에게 장치(예를 들면 90의 경사각을 가진 두거울)를 제공하여, 같은 시각에 A와 B두곳을 한 꺼번에 관찰할 수 있게 한다.\n","\n","정답 : \"먼저 약속을 어긴 쪽은 한달준 그 놈이었어.\"\n","출력 : \"먼저약속을 어긴 쪽은 한 달준 그 놈이었어.\"\n","\n","정답 : 레이첼.\n","출력 : 레이첼.\n","\n","정답 : 처남은 의사의 진단서를 북북 찢어버렸다.\n","출력 : 처남은 의 사의 진단서를 북북찢어버렸다.\n","\n","정답 : 그 전화를 받았던 날, 과일을 벗기던 혜숙이가 물었었다.\n","출력 : 그 전화를 받았던 날, 과 일을 벗기던 혜숙이가 물었었다.\n","\n","정답 : \"저도 처음에는 금변호사님처럼 그렇게 생각했지요. 그러나 범인은 경계선을 사이에 두고 한쪽 발은 테이블쪽에 두고 한쪽 발을 홀 중앙에 두고 있던 있던 게 분명해요.\"\n","출력 : \"저도 처음에는 금변호사님처럼 그렇게 생각했지요. 그러나 범인은 경계선을 사이에 두고 한 쪽발은 테이 블쪽에 두고 한 쪽발을 홀중앙에 두고 있던 있던게 분명해요.\"\n","\n","정답 : 1979년 <모두를 위한 정의>로 마지막 오스카 주연상 후보에 올랐었으나 이 영화를 제외하고는 70년대 말과 80년대 초를 통틀어 별로 신통한 영화에는 나오지 않았다.\n","출력 : 1979년 <모두를 위한 정의 >로 마지 막 오스카주연상후보에 올랐었으나이 영화를 제외하고는 70년대말과 80년대초를 통틀어별로 신통한 영화에는 나오지 않았다.\n","\n","정답 : 경찰은 처음에 그 사건이 아그자 혼자 저지른 단독 범행인 줄 알았지만 조사 결과 두 명 이상의 공범이 있었음이 밝혀졌고, 수사가 진전됨에 따라 그 배후에는 여러 나라의 테러조직과 정보기관들이 난마처럼 얽혀 있음이 드러났다.\n","출력 : 경찰은 처음에 그사건이 아 그자 혼자저지른 단독범행인 줄 알았지만 조사결과 두명이 상의 공범이 있었음이 밝혀졌고, 수사가 진전됨에 따라 그배후에는 여러나라의 테러조직과 정보기관들이난 마처럼 얽혀 있음이드러났다.\n","\n","Before Accuracy : 0.8900032351989647\n","\n","\n","[개선 후]\n","\n","정답 : 그러고 보니 경리는 윤보혜의 근황에 대해 아는 것이 없어보였다.\n","출력 : 그러고 보니 경리는 윤보혜의 근황에 대해 아는 것이 없어 보였다.\n","\n","정답 : 이제 7년 대환란이 눈앞에 닥쳐왔습니다.\n","출력 : 이제7년 대환란이 눈앞에 닥쳐왔습니다.\n","\n","정답 : 이 관찰자에게 장치(예를 들면 90의 경사각을 가진 두 거울)를 제공하여, 같은 시각에 A와 B 두 곳을 한꺼번에 관찰할 수 있게 한다.\n","출력 : 이관찰자에 게 장치(예를 들면 90의 경사각을 가진 두 거울)를 제공하여, 같은 시각에 A와 B두 곳을 한꺼번에 관찰할 수 있게 한다.\n","\n","정답 : \"먼저 약속을 어긴 쪽은 한달준 그 놈이었어.\"\n","출력 : \"먼저 약속을 어긴 쪽은 한달준 그 놈이었어.\"\n","\n","정답 : 레이첼.\n","출력 : 레이첼.\n","\n","정답 : 처남은 의사의 진단서를 북북 찢어버렸다.\n","출력 : 처남은 의사의 진단서를 북북 찢어버렸다.\n","\n","정답 : 그 전화를 받았던 날, 과일을 벗기던 혜숙이가 물었었다.\n","출력 : 그 전화를 받았던 날, 과일을 벗기던 혜숙이가 물었었다.\n","\n","정답 : \"저도 처음에는 금변호사님처럼 그렇게 생각했지요. 그러나 범인은 경계선을 사이에 두고 한쪽 발은 테이블쪽에 두고 한쪽 발을 홀 중앙에 두고 있던 있던 게 분명해요.\"\n","출력 : \"저도 처음에는 금변호사님처럼 그렇게 생각했지요. 그러나 범인은 경계선을 사이에 두고 한 쪽발은 테이 블쪽에 두고 한 쪽발을 홀중앙에 두고 있던 있던게 분명해요.\"\n","\n","정답 : 1979년 <모두를 위한 정의>로 마지막 오스카 주연상 후보에 올랐었으나 이 영화를 제외하고는 70년대 말과 80년대 초를 통틀어 별로 신통한 영화에는 나오지 않았다.\n","출력 : 1979년 <모두를 위한 정의>로 마지막오스카 주연상후보에 올랐었으나 이영화를 제외하고는 70년 대말과 80년 대초를 통틀어 별로 신통한 영화에는 나오지 않았다.\n","\n","정답 : 경찰은 처음에 그 사건이 아그자 혼자 저지른 단독 범행인 줄 알았지만 조사 결과 두 명 이상의 공범이 있었음이 밝혀졌고, 수사가 진전됨에 따라 그 배후에는 여러 나라의 테러조직과 정보기관들이 난마처럼 얽혀 있음이 드러났다.\n","출력 : 경찰은 처음에 그 사건이아그 자 혼자 저지른단 독범행인 줄 알았지만 조사결과 두명이상의 공범이 있었음이 밝혀졌고, 수사가 진 전됨에 따라 그 배후에는 여러나라의 테러조직과 정보기관들이 난마처럼 얽혀 있음이 드러났다.\n","\n","Improved Accuracy : 0.9158848269168554\n"]}]}]}
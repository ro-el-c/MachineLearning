{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOm5CR7bq7I+59j0EM8Sf0u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-peQMWULHkM9","executionInfo":{"status":"ok","timestamp":1667893811405,"user_tz":-540,"elapsed":19166,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}},"outputId":"071ca11a-0b6e-4a75-93cb-715b82b693db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n","\n","class CIFAR10_CNN(nn.Module):\n","  def __init__(self, config):\n","    super(CIFAR10_CNN, self).__init__()\n","\n","    # 첫번째 층: Convolutional NN\n","    # (batch, 32, 32, 3) -> (batch, 32, 32, 9) -> (batch, 16, 16, 9)\n","    self.conv1 = nn.Sequential()\n","    self.conv1.add_module('conv1', nn.Conv2d(3, 9, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1)))\n","    self.conv1.add_module('relu', nn.ReLU())\n","    self.conv1.add_module('maxpool1', nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n","\n","    # 두번째 층: Convolutional NN\n","    # (batch, 16, 16, 9) -> (batch, 16, 16, 72) -> (batch, 8, 8, 72)\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(9, 72, kernel_size = 3, stride = 1, padding = 1),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size = 2, stride = 2))\n","    \n","    # 네번째 층: Fully-Connected NN\n","    # (batch, 8, 8, 72) -> (batch, 10)\n","    self.fnn = nn.Linear(8*8*72, 10, bias = True)\n","\n","    # FNN 가중치 초기화\n","    nn.init.xavier_uniform_(self.fnn.weight)\n","\n","  def forward(self, input_features):\n","\n","    # Convolution\n","    output = self.conv1(input_features)\n","    output = self.conv2(output)\n","\n","    # 텐서를 1차원으로 펼치기: (batch, -1)\n","    # output.size(0): 배치 차원의 크기, -1: 해당 차원은 파이토치가 알아서 설정\n","    output = output.view(output.size(0), -1)\n","    hypothesis = self.fnn(output)\n","   \n","    return hypothesis"],"metadata":{"id":"2CTtPwLoHrDW","executionInfo":{"status":"ok","timestamp":1667896161713,"user_tz":-540,"elapsed":472,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# batch file to dict\n","def batch2dict(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","# 데이터 읽기\n","def load_dataset():\n","  batch_1 = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/data_batch_1\"\n","  batch_2 = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/data_batch_2\"\n","  batch_3 = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/data_batch_3\"\n","  batch_4 = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/data_batch_4\"\n","  batch_5 = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/data_batch_5\"\n","  test_batch = \"/gdrive/MyDrive/ml_colab/week10/cifar-10-batches-py/test_batch\"\n","\n","  batch_1_data = batch2dict(batch_1)[b'data'].reshape(-1,3,32,32)\n","  batch_2_data = batch2dict(batch_2)[b'data'].reshape(-1,3,32,32)\n","  batch_3_data = batch2dict(batch_3)[b'data'].reshape(-1,3,32,32)\n","  batch_4_data = batch2dict(batch_4)[b'data'].reshape(-1,3,32,32)\n","  batch_5_data = batch2dict(batch_5)[b'data'].reshape(-1,3,32,32)\n","  test_data = batch2dict(test_batch)[b'data'].reshape(-1,3,32,32)\n","\n","  batch_1_labels = batch2dict(batch_1)[b'labels']\n","  batch_2_labels = batch2dict(batch_2)[b'labels']\n","  batch_3_labels = batch2dict(batch_3)[b'labels']\n","  batch_4_labels = batch2dict(batch_4)[b'labels']\n","  batch_5_labels = batch2dict(batch_5)[b'labels']\n","  test_labels = batch2dict(test_batch)[b'labels']\n","\n","  batch_1_labels.extend(batch_2_labels)\n","  batch_1_labels.extend(batch_3_labels)\n","  batch_1_labels.extend(batch_4_labels)\n","  batch_1_labels.extend(batch_5_labels)\n","\n","  train_X = np.vstack((batch_1_data, batch_2_data, batch_3_data, batch_4_data, batch_5_data))\n","  train_y = batch_1_labels\n","  test_X = test_data\n","  test_y = test_labels\n","  print(train_X.shape) # (50000, 3, 32, 32)\n","  print(test_X.shape) # (10000, 3, 32, 32)\n","\n","  train_X = torch.tensor(train_X, dtype=torch.float)\n","  train_y = torch.tensor(train_y, dtype=torch.long)\n","  test_X = torch.tensor(test_X, dtype=torch.float)\n","  test_y = torch.tensor(test_y, dtype=torch.long)\n","  \n","  return (train_X, train_y), (test_X, test_y)"],"metadata":{"id":"RszWwGZ_Kma_","executionInfo":{"status":"ok","timestamp":1667896163914,"user_tz":-540,"elapsed":2,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# 모델 평가 결과 계산을 위해 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","# 평가 수행 함수\n","def do_test(model, test_dataloader):\n","\n","  # 평가 모드 setting\n","  model.eval()\n","\n","  # batch 별로 예측값과 정답을 저장할 리스트 초기화\n","  predicts, golds = [], []\n","  \n","  with torch.no_grad():\n","    for step, batch in enumerate(test_dataloader):\n","  \n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      input_features, labels = batch\n","      hypothesis = model(input_features)\n","\n","      # ont-hot 표현으로 변경\n","      logits = torch.argmax(hypothesis, -1)\n","\n","      x = tensor2list(logits)\n","      y = tensor2list(labels)\n","\n","      # 예측값과 정답을 리스트에 추가\n","      predicts.extend(x)\n","      golds.extend(y)\n","    \n","    print(\"PRED=\",predicts)\n","    print(\"GOLD=\",golds)\n","    print(\"Accuracy= {0:f}\\n\".format(accuracy_score(golds, predicts)))\n","\n","# 모델 평가 함수\n","def test(config):\n","  model = CIFAR10_CNN(config).cuda()\n","\n","  # 저장된 모델 가중치 로드\n","  model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"model_name\"])))\n","\n","  # 데이터 load\n","  (_, _), (features, labels) = load_dataset()\n","  \n","  test_features = TensorDataset(features, labels)\n","  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n","  \n","  do_test(model, test_dataloader)"],"metadata":{"id":"r-nyCBSdNIdC","executionInfo":{"status":"ok","timestamp":1667896167045,"user_tz":-540,"elapsed":2,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 함수\n","def train(config):\n","  # 모델 생성\n","  model = CIFAR10_CNN(config).cuda()\n","\n","  # 데이터 읽기\n","  (input_features, labels), (_, _) = load_dataset()\n","\n","  # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n","  train_features = TensorDataset(input_features, labels)\n","  train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","  # 크로스엔트로피 비용 함수 \n","  loss_func = nn.CrossEntropyLoss()\n","  # 옵티마이저 함수 (역전파 알고리즘을 수행할 함수)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n","\n","  for epoch in range(config[\"epoch\"]+1):\n","    # 학습 모드 setting\n","    model.train()\n","  \n","    # epoch 마다 평균 비용을 저장하기 위한 리스트\n","    costs = []\n","\n","    for (step, batch) in enumerate(train_dataloader):\n","\n","      # batch = (input_features[step], labels[step])*batch_size\n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      # 각 feature 저장\n","      input_features, labels = batch\n","\n","      # 역전파 변화도 초기화\n","      # .backward() 호출 시, 변화도 버퍼에 데이터가 계속 누적한 것을 초기화\n","      optimizer.zero_grad()\n","\n","      # H(X) 계산: forward 연산\n","      hypothesis = model(input_features)\n","      # 비용 계산\n","      cost = loss_func(hypothesis, labels)\n","      # 역전파 수행\n","      cost.backward()\n","      optimizer.step()\n","   \n","      # 현재 batch의 스텝 별 loss 저장\n","      costs.append(cost.data.item())\n","    \n","    # 에폭마다 평균 비용 출력하고 모델을 저장\n","    print(\"epoch {0} - Average Loss= {1:f}\".format(epoch, np.mean(costs)))\n","    torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))\n","    do_test(model, train_dataloader)"],"metadata":{"id":"5COy4NDSNZDC","executionInfo":{"status":"ok","timestamp":1667896169515,"user_tz":-540,"elapsed":3,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/ml_colab/week10/cifar10\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"learn_rate\":0.001,\n","              \"batch_size\":32,\n","              \"epoch\":20,\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1cJcWJgb5kDWAPLqnxFoNnjLLWGVaPsh_"},"id":"lVe0Gc75NftS","executionInfo":{"status":"ok","timestamp":1667896293726,"user_tz":-540,"elapsed":121554,"user":{"displayName":"alcxzp","userId":"00037678357315986784"}},"outputId":"dc289542-f736-44ca-b209-238378a1576d"},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}